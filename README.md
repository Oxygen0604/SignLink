# SignLink · 图像识别手语翻译系统

SignLink 是一个集成图像识别与 3D 可视化的手语、盲文翻译系统，支持**手语盲文实时识别翻译、视频图像翻译**以及**自然语言文本转手语动画、盲文图像**功能，致力于打造健听人士与听障人士之间无障碍沟通的智能桥梁。

---

## 项目简介

SignLink 提供了**即时翻译能力**：

- **手语 -> 文字 -> 盲文**：通过摄像头实时捕捉手势，识别手语并翻译为文字。也支持上传视频或图片翻译。将文字转化为盲文坐标序列并在硬件、虚拟机中反馈

---

## 核心功能

-  **实时摄像头识别**：通过设备摄像头捕捉手势动作、麦克风捕捉对话信息，实时进行翻译。
-  **视频/图片上传识别**：上传含有手语的视频或图片，系统识别并输出对应文字翻译。
-  **语音听写**： 实时语音听写并在前端显示。
-  **语义映射与自然语言处理**：集成大模型与 LSTM 网络，实现手语、盲文到自然语言之间的高质量语义转换。
-  **多平台支持**：支持 Android、iOS 和 Web 端部署。

---
## 核心功能实现方案

### 本地手语识别流程

- **前端** 使用React Native摄像头API直接调用手机摄像头，实时采集视频流
- **数据传输** 将视频帧或视频流传给后端API
- **后端** 使用MediaPipe进行手部关键点检测，调用本地训练好的手语识别模型
- **AI处理** 将手语序列转换为自然语言文本
- **结果返回** 将翻译结果返回给前端显示

### 视频/图片上传识别

- **前端** 支持从相册选择或直接拍摄视频/图片
- **后端** 接收上传的文件，使用OpenCV和MediaPipe进行预处理和手部关键点检测
- **模型推理** 调用本地手语识别模型进行翻译
- **结果展示** 将识别结果返回前端进行可视化展示

### 语音听写（待实现）

- **前端** 利用设备麦克风采集音频
- **AI处理** 语音识别转文字（计划集成百度/腾讯/阿里云等语音识别API）
- **结果展示** 实时显示转录文本

### 盲文转换（待实现）
- **文本处理** 将识别的文字转换为盲文坐标序列
- **硬件反馈** 支持在虚拟环境和硬件设备中反馈盲文信息


---

## 技术栈

| 层级       | 技术方案                           |
|------------|----------------------------------|
| 前端       | React Native 0.81, TypeScript     |
| 后端       | Python, FastAPI                   |
| 图像识别   | MediaPipe, OpenCV                 |
| 机器学习   | TensorFlow/Keras, LSTM            |
| 实时通信   | HTTP API, WebSocket (预留)         |
| 渲染引擎   | Three.js（3D 手势动画）              |  
---

## 项目目录结构

SignLink/  
├── frontend/ # React Native 前端代码  
│ ├── pages/  
│ │ ├── SignToText/ # 手语翻译为文字页  
│ │ └── TextToSign/ # 文字翻译为手语页  
│ ├── components/  
│ │ └── HandModel3D/ # Three.js 3D 模型组件  
│ └── assets/  
├── backend/ # Python 后端服务  
│ ├── app.py  
│ ├── mediapipe_module/ # 图像识别  
│ └── nlp_module/ # LSTM + 语义模型  
├── public/ # 公共图像/视频资源  
├── models/ # 模型权重文件  
├── requirements.txt # Python 依赖  
└── README.md  

---

## 快速开始

### 环境要求
- Python 3.8+
- Node.js 20+
- Android Studio / Xcode (移动端开发)

### 后端运行
```bash
# 安装Python依赖
cd backend/
pip install -r requirements.txt

# 启动后端服务
cd app/
uvicorn main:app --host 0.0.0.0 --port 8000
```

### 前端运行（React Native）
```bash
# 安装依赖
cd frontend/
npm install

# 启动项目
npm start

# 运行在Android设备
npm run android

# 运行在iOS设备
npm run ios
```

### AI模型训练（如需重新训练）
```bash
cd ai_services/set_training_translation/
# 按README.md中的步骤进行数据采集和模型训练
python train_sign_language_model.py
```  

---

## 应用场景
- **聋哑人士日常交流沟通** SignLink提供了高效、即时的语言转化功能。摄像头能实时识别用户手语并转化为自然语言，并能听写自然人所说的话并显示在前端页面中供用户观看，能极大程度的促进聋哑人士日常生活中的正常沟通
- **手语、盲文学习交流** SignLink提供了准确的手语、盲文翻译功能，便于手语、盲文的学习

## 开发阶段
1. ✅ 完善React Native前端界面，实现摄像头调用和视频流处理
2. ✅ 完成手语识别模型训练和部署
3. ✅ 实现后端API，处理视频流并返回识别结果
4. 🔄 优化手语识别准确率，增加更多手势词汇
5. 📋 实现语音听写功能（集成云端语音识别API）
6. 📋 实现文字转盲文功能，支持盲文坐标序列输出
7. 📋 尝试盲文板硬件接入（如有需要）


## 贡献者
    
......文档待补充