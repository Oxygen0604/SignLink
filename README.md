# SignLink · 智能手语翻译与学习系统

SignLink 是一个集成实时手语识别、AI问答及互动学习的跨平台移动应用，支持**实时摄像头识别翻译、手语AI问答**以及**互动学习答题**功能，致力于打造健听人士与听障人士之间无障碍沟通的智能桥梁。

---

## 项目简介

SignLink 提供了**完整的手语沟通与学习解决方案**：

- **实时手语翻译**：通过摄像头实时捕捉手势，识别手语并翻译为文字/语音
- **手语AI问答**：支持通过手语提问，AI实时生成手语/文字回答
- **互动学习答题**：设计分级答题系统，包含手语词汇练习、句子理解、场景模拟等题型
- **多平台支持**：支持 Android 和 iOS 移动端部署

---

## 核心功能

### 1. 实时翻译模块
- **摄像头实时识别**：通过设备摄像头捕捉手势动作，实时进行手语到文字/语音的翻译
- **视频/图片上传识别**：上传含有手语的视频或图片，系统识别并输出对应文字翻译
- **语音转文字**：实时语音听写并在前端显示，支持双向沟通

### 2. 手语AI问答模块
- **手语提问识别**：识别用户的手语提问
- **AI智能回答**：集成大模型，生成自然语言回答
- **手语动画展示**：将文字回答转换为手语动画展示

### 3. 互动学习答题模块
- **分级答题系统**：包含基础词汇、句子理解、场景模拟等多种题型
- **实时反馈**：答题后立即获得反馈和解析
- **学习记录**：记录学习进度和成绩
- **排行榜**：激励用户学习，展示学习排名

### 4. 用户认证系统
- **注册登录**：支持邮箱注册和登录
- **个人信息管理**：用户可以管理个人信息
- **登录状态持久化**：自动保存登录状态，下次启动无需重新登录

---

## 核心功能实现方案

### 实时手语识别流程

1. **前端** 使用React Native摄像头API直接调用手机摄像头，实时采集视频流
2. **数据传输** 将视频帧或视频流传给后端API
3. **后端** 使用MediaPipe进行手部关键点检测，调用本地训练好的手语识别模型
4. **AI处理** 将手语序列转换为自然语言文本
5. **结果返回** 将翻译结果返回给前端显示

### 手语AI问答流程

1. **前端** 采集用户手语提问
2. **后端** 识别手语并转换为文字
3. **AI处理** 调用大模型生成回答
4. **结果返回** 将文字回答返回前端展示

### 互动学习答题流程

1. **前端** 展示题目和选项
2. **用户交互** 用户通过手语或点击选择答案
3. **结果判断** 后端判断答案正确性
4. **反馈展示** 前端展示结果和解析
5. **数据存储** 保存学习记录到数据库

---

## 技术栈

| 层级       | 技术方案                           |
|------------|----------------------------------|
| 前端框架   | React Native 0.81, TypeScript     |
| 状态管理   | Zustand                          |
| 导航管理   | React Navigation                 |
| 后端框架   | Node.js                          |
| 数据库     | MongoDB                          |
| 图像识别   | MediaPipe, OpenCV                 |
| 机器学习   | TensorFlow/Keras, LSTM            |
| 实时通信   | HTTP API, WebSocket (预留)         |
| 渲染引擎   | Three.js（3D 手势动画）              |  
---

## 项目目录结构

SignLink/  
├── ai_services/ # AI服务相关代码  
├── backend/ # Node.js后端服务  
├── docs/ # 项目文档  
├── frontend/ # React Native前端代码  
│   ├── assets/ # 静态资源文件  
│   ├── src/ # 源代码  
│   │   ├── api/ # API请求封装  
│   │   ├── components/ # 通用组件  
│   │   ├── screens/ # 页面组件  
│   │   │   ├── AuthScreen/ # 认证相关页面  
│   │   │   ├── HomeScreen/ # 首页  
│   │   │   ├── SignAIScreen/ # 手语AI问答  
│   │   │   ├── SignAnswerScreen/ # 互动学习答题  
│   │   │   └── SignTransScreen/ # 手语翻译  
│   │   ├── store/ # 状态管理  
│   │   ├── types/ # TypeScript类型定义  
│   │   └── utils/ # 工具函数  
│   ├── ios/ # iOS平台相关代码  
│   └── android/ # Android平台相关代码  
├── node_modules/ # 依赖包  
├── .gitignore # Git忽略文件  
├── package.json # 项目配置文件  
├── yarn.lock # Yarn依赖锁文件  
└── README.md # 项目说明文档  

---

## 快速开始

### 环境要求
- Node.js 20+
- Yarn 1.22+
- Android Studio / Xcode (移动端开发)

### 前端运行（React Native）

```bash
# 安装依赖
cd frontend/
yarn install

# 启动开发服务器
yarn start

# 运行在Android设备
yarn android

# 运行在iOS设备
yarn ios
```

### 后端运行

```bash
# 安装依赖
cd backend/
yarn install

# 启动后端服务
yarn start
```

### AI模型训练（如需重新训练）

```bash
cd ai_services/
# 按README.md中的步骤进行数据采集和模型训练
python train_sign_language_model.py
```  

---

## 应用场景

### 日常沟通场景
- **听障人士与健听人士沟通**：SignLink提供了高效、即时的语言转化功能，摄像头能实时识别用户手语并转化为自然语言，同时能将健听人士的语音转化为文字供听障用户观看，极大程度地促进听障人士日常生活中的正常沟通
- **特殊环境沟通**：在嘈杂环境或需要安静的场合，使用手语翻译功能进行沟通

### 学习教育场景
- **手语学习**：SignLink提供了准确的手语识别和翻译功能，便于手语的学习和练习
- **特殊教育辅助**：作为特殊教育学校的教学辅助工具，帮助听障学生学习和练习手语
- **健听人士学习手语**：方便健听人士学习手语，促进无障碍沟通

---

## 开发阶段

1. ✅ 完成React Native前端界面开发
2. ✅ 实现实时摄像头调用和视频流处理
3. ✅ 完成手语识别模型训练和部署
4. ✅ 实现后端API，处理视频流并返回识别结果
5. ✅ 实现手语AI问答功能
6. ✅ 实现互动学习答题功能
7. ✅ 实现用户认证系统
8. 🔄 优化手语识别准确率，增加更多手势词汇
9. 📋 实现文字转手语动画功能
10. 📋 扩展多语言支持

---

## 贡献者

- **前端开发**：负责React Native跨平台应用开发
- **后端开发**：负责Node.js后端服务和API设计
- **AI算法开发**：负责手语识别模型训练和优化
- **UI/UX设计**：负责应用界面设计和用户体验优化

---

## 许可证

MIT License

---

## 联系方式

如有问题或建议，请通过以下方式联系我们：
- Email: 13308839061@foxmail.com
- GitHub Issues: [https://github.com/Oxygen0604/SignLink/issues](https://github.com/Oxygen0604/SignLink/issues)

---

## 更新日志

### v1.0.0 (2025-12-25)
- 完成基础功能开发
- 实现实时手语翻译功能
- 实现手语AI问答功能
- 实现互动学习答题功能
- 实现用户认证系统
- 支持Android和iOS平台

### 未来规划
- 优化手语识别准确率
- 增加更多手势词汇
- 实现文字转手语动画功能
- 扩展多语言支持
- 增加社区功能，支持用户交流和分享
- 实现Web端部署

---

## 致谢

感谢所有为SignLink项目做出贡献的开发者和志愿者，以及测试用户的反馈和建议。我们将继续努力，不断优化产品，为听障人士和手语学习者提供更好的服务。