# 手语手势识别系统 - 使用指南

## 🎯 项目简介

这是一个基于深度学习的手语手势识别系统,使用 CNN+RNN 架构来识别视频中的手语手势。

## ✅ 已完成的工作

1. ✅ 安装所有必需的依赖
2. ✅ 从视频中提取帧并进行手部分割
3. ✅ 训练 CNN 模型(MobileNetV2)进行空间特征提取
4. ✅ 使用 CNN 提取视频帧的特征
5. ✅ 训练 RNN 模型(LSTM)进行时序建模
6. ✅ 在测试集上评估模型并达到 100%准确率

## 📊 模型性能

- **CNN 模型**: 验证准确率 100%
- **RNN 模型**: 验证准确率 100%
- **测试集准确率**: 100% (6/6 个视频全部预测正确)

## 📁 重要文件说明

### 模型文件

- `gesture_model_best.h5` - 训练好的 CNN 模型
- `checkpoints/gesture_rnn.h5` - 训练好的 RNN 模型
- `retrained_labels.txt` - 类别标签文件

### 数据文件

- `train_frames/` - 训练视频的帧
- `test_frames/` - 测试视频的帧
- `predicted-frames-final_result-train.pkl` - 训练特征
- `predicted-frames-final_result-test.pkl` - 测试特征

### 脚本文件（TensorFlow 2.x 版本）

- `retrain_tf2.py` - 训练 CNN 模型
- `predict_spatial_tf2.py` - 提取视频帧特征
- `rnn_train_tf2.py` - 训练 RNN 模型
- `rnn_eval_tf2.py` - 评估 RNN 模型
- `predict_video.py` - **对新视频进行预测（推理脚本）**

## 🚀 如何使用训练好的模型

### 对新视频进行预测

```bash
python predict_video.py <视频文件路径>
```

**示例:**

```bash
# 预测Accept手势
python predict_video.py test_videos/Accept/050_010_001.mp4

# 预测Appear手势
python predict_video.py test_videos/Appear/053_010_001.mp4
```

**输出示例:**

```
类别: ['Accept', 'Appear']

加载CNN模型...
加载RNN模型...

从视频提取帧: test_videos/Accept/050_010_001.mp4
提取了 201 帧
预处理帧...
使用CNN提取特征...
使用RNN进行预测...

==================================================
预测结果:
==================================================
手势: Accept
置信度: 0.5195
==================================================

所有类别的概率:
  Accept: 0.5195
  Appear: 0.4805
```

## 🔄 如何重新训练模型

如果您想用新数据重新训练模型,请按以下步骤操作:

### 步骤 1: 准备数据

将视频按类别放入`train_videos`和`test_videos`文件夹:

```
train_videos/
├── 类别1/
│   ├── video1.mp4
│   ├── video2.mp4
│   └── ...
└── 类别2/
    ├── video1.mp4
    └── ...
```

### 步骤 2: 提取帧

```bash
python video-to-frame.py train_videos train_frames
python video-to-frame.py test_videos test_frames
```

### 步骤 3: 训练 CNN 模型

```bash
python retrain_tf2.py --image_dir train_frames --epochs 15
```

### 步骤 4: 提取特征

```bash
# 训练集
python predict_spatial_tf2.py gesture_model_best.h5 train_frames --batch_size 100

# 测试集
python predict_spatial_tf2.py gesture_model_best.h5 test_frames --batch_size 100 --test
```

### 步骤 5: 训练 RNN 模型

```bash
python rnn_train_tf2.py predicted-frames-final_result-train.pkl gesture_rnn.model --batch_size 32 --epochs 20
```

### 步骤 6: 评估模型

```bash
python rnn_eval_tf2.py predicted-frames-final_result-test.pkl gesture_rnn.model
```

## 📦 依赖项

所有依赖已安装在`requirements.txt`中:

```
opencv-python
opencv-contrib-python
tensorflow
numpy
scikit-learn
tqdm
h5py
```

## 💡 提示和建议

### 提高模型性能

1. **增加训练数据** - 更多的视频样本可以提高泛化能力
2. **数据增强** - 对视频进行旋转、缩放、亮度调整等
3. **调整超参数** - 尝试不同的学习率、批大小、epoch 数
4. **尝试不同架构** - 可以尝试 Inception V3 或 ResNet 作为特征提取器

### 使用不同的模型架构

RNN 训练脚本支持三种架构:

```bash
# 默认架构
python rnn_train_tf2.py ... --model_type default

# 宽LSTM (推荐)
python rnn_train_tf2.py ... --model_type wide

# 深LSTM
python rnn_train_tf2.py ... --model_type deep
```

### 使用特征层而非 Softmax

如果想使用 CNN 的特征层(而不是 softmax 输出):

```bash
python predict_spatial_tf2.py gesture_model_best.h5 train_frames --use_features
```

## 🔍 查看结果

- 训练过程的详细信息保存在终端输出中
- 测试结果保存在`results.txt`文件中
- 完整的训练总结在`TRAINING_SUMMARY.md`中

## ❓ 常见问题

**Q: 为什么有两套脚本(原始的和 tf2 版本)?**
A: 原始脚本使用 TFLearn,与 TensorFlow 2.x 不兼容。我创建了新的 tf2 版本脚本来替代它们。

**Q: 能识别多少种手势?**
A: 当前模型训练了 2 种手势(Accept 和 Appear)。您可以通过添加更多类别的训练数据来扩展。

**Q: 预测一个视频需要多长时间?**
A: 在 CPU 上大约需要 10-15 秒,包括帧提取、特征提取和预测。

**Q: 如何添加新的手势类别?**
A: 在`train_videos`和`test_videos`中创建新的文件夹,放入相应的视频,然后重新训练模型。

## 📝 技术细节

### 模型架构

1. **空间特征提取**: MobileNetV2 → 每帧提取 2 维 softmax 概率
2. **时序建模**: LSTM(256 单元) → 学习 201 帧的时序关系
3. **分类**: 全连接层 → 输出最终手势类别

### 数据流程

```
视频 → 帧提取(201帧) → CNN特征提取 → RNN时序建模 → 预测结果
```

## 🎉 成功案例

测试集所有 6 个视频全部正确识别:

- Accept 类别: 3/3 正确
- Appear 类别: 3/3 正确

## 📧 联系与支持

如有问题,请参考:

- 原始论文: https://link.springer.com/chapter/10.1007/978-981-10-7566-7_63
- GitHub 仓库: https://github.com/hthuwal/sign-language-gesture-recognition

---

**祝您使用愉快!** 🎊
